{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f52e4b4-5371-4abe-bc67-1ebbadf7099d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "# Deep Neural Network: From Scratch\n",
    "\n",
    "## **Introduction**\n",
    "A **Deep Neural Network (DNN)** is a neural network with multiple hidden layers. It is capable of solving more complex problems than a simple neural network. In this notebook, we will implement a deep neural network from scratch using NumPy.\n",
    "\n",
    "---\n",
    "\n",
    "## **Mathematical Foundations**\n",
    "\n",
    "### **1. Forward Propagation**\n",
    "Forward propagation involves computing the output of the network given an input. The steps are as follows:\n",
    "\n",
    "1. **Linear Transformation (Hidden Layer \\\\( i \\\\)):**\n",
    "\n",
    "\n",
    "   \\\\[\n",
    "   z_i = a_{i-1} \\cdot W_i + b_i\n",
    "   \\\\]\n",
    "\n",
    "   \n",
    "   - \\\\( a_{i-1} \\\\): Activation of the previous layer.\n",
    "   - \\\\( W_i \\\\): Weights of the current layer.\n",
    "   - \\\\( b_i \\\\): Biases of the current layer.\n",
    "\n",
    "3. **Activation Function (Hidden Layer \\\\( i \\\\)):**\n",
    "   \\\\[\n",
    "   a_i = \\sigma(z_i)\n",
    "   \\\\]\n",
    "   - \\\\( \\sigma \\\\): Sigmoid activation function.\n",
    "\n",
    "4. **Output Layer:**\n",
    "   \\\\[\n",
    "   z_{\\text{out}} = a_{\\text{last}} \\cdot W_{\\text{out}} + b_{\\text{out}}\n",
    "   \\\\]\n",
    "   \\\\[\n",
    "   a_{\\text{out}} = \\sigma(z_{\\text{out}})\n",
    "   \\\\]\n",
    "\n",
    "### **2. Backpropagation**\n",
    "Backpropagation involves computing the gradients of the loss function with respect to the weights and biases. The steps are as follows:\n",
    "\n",
    "1. **Compute the Error:**\n",
    "   \\\\[\n",
    "   \\text{error} = a_{\\text{out}} - y\n",
    "   \\\\]\n",
    "\n",
    "2. **Compute the Gradient of the Loss with Respect to \\\\( W_{\\text{out}} \\\\) and \\\\( b_{\\text{out}} \\\\):**\n",
    "   \\\\[\n",
    "   \\delta_{\\text{out}} = \\text{error} \\cdot \\sigma'(z_{\\text{out}})\n",
    "   \\\\]\n",
    "   \\\\[\n",
    "   \\frac{\\partial L}{\\partial W_{\\text{out}}} = a_{\\text{last}}^T \\cdot \\delta_{\\text{out}}\n",
    "   \\\\]\n",
    "   \\\\[\n",
    "   \\frac{\\partial L}{\\partial b_{\\text{out}}} = \\sum \\delta_{\\text{out}}\n",
    "   \\\\]\n",
    "\n",
    "3. **Compute the Gradient of the Loss with Respect to \\\\( W_i \\\\) and \\\\( b_i \\\\):**\n",
    "   \\\\[\n",
    "   \\delta_i = \\delta_{i+1} \\cdot W_{i+1}^T \\cdot \\sigma'(z_i)\n",
    "   \\\\]\n",
    "   \\\\[\n",
    "   \\frac{\\partial L}{\\partial W_i} = a_{i-1}^T \\cdot \\delta_i\n",
    "   \\\\]\n",
    "   \\\\[\n",
    "   \\frac{\\partial L}{\\partial b_i} = \\sum \\delta_i\n",
    "   \\\\]\n",
    "\n",
    "4. **Update the Weights and Biases:**\n",
    "   \\\\[\n",
    "   W_i = W_i - \\eta \\cdot \\frac{\\partial L}{\\partial W_i}\n",
    "   \\\\]\n",
    "   \\\\[\n",
    "   b_i = b_i - \\eta \\cdot \\frac{\\partial L}{\\partial b_i}\n",
    "   \\\\]\n",
    "\n",
    "---\n",
    "\n",
    "## **Implementation**\n",
    "Below is the Python code for implementing a deep neural network from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bdbb177-2cf1-4a31-afcb-95dcf608da3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions after training:\n",
      "Input: [0 0 1], Output: [0.00394415]\n",
      "Input: [0 1 1], Output: [0.99481365]\n",
      "Input: [1 0 1], Output: [0.99459514]\n",
      "Input: [1 1 1], Output: [0.00561824]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DeepNeuralNetwork:\n",
    "    def __init__(self, layer_sizes):\n",
    "        # Initialize weights and biases for each layer\n",
    "        self.weights = [np.random.randn(layer_sizes[i], layer_sizes[i+1]) for i in range(len(layer_sizes)-1)]\n",
    "        self.biases = [np.random.randn(layer_sizes[i+1]) for i in range(len(layer_sizes)-1)]\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        # Sigmoid activation function\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        # Derivative of the sigmoid function\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Forward propagation\n",
    "        self.activations = [X]\n",
    "        self.z_values = []\n",
    "        for i in range(len(self.weights)):\n",
    "            z = np.dot(self.activations[-1], self.weights[i]) + self.biases[i]\n",
    "            self.z_values.append(z)\n",
    "            self.activations.append(self.sigmoid(z))\n",
    "        return self.activations[-1]\n",
    "\n",
    "    def backward(self, X, y, output):\n",
    "        # Backpropagation\n",
    "        self.error = output - y\n",
    "        self.deltas = [self.error * self.sigmoid_derivative(output)]\n",
    "        for i in range(len(self.weights)-1, 0, -1):\n",
    "            delta = np.dot(self.deltas[-1], self.weights[i].T) * self.sigmoid_derivative(self.activations[i])\n",
    "            self.deltas.append(delta)\n",
    "        self.deltas.reverse()\n",
    "\n",
    "        # Update weights and biases\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= np.dot(self.activations[i].T, self.deltas[i])\n",
    "            self.biases[i] -= np.sum(self.deltas[i], axis=0)\n",
    "\n",
    "    def train(self, X, y, epochs=1000):\n",
    "        # Training the network\n",
    "        for _ in range(epochs):\n",
    "            output = self.forward(X)\n",
    "            self.backward(X, y, output)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Input data (4 samples, 3 features each)\n",
    "    X = np.array([[0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]])\n",
    "    y = np.array([[0], [1], [1], [0]])  # XOR operation\n",
    "\n",
    "    # Create a deep neural network\n",
    "    nn = DeepNeuralNetwork(layer_sizes=[3, 4, 4, 1])\n",
    "\n",
    "    # Train the network\n",
    "    nn.train(X, y, epochs=10000)\n",
    "\n",
    "    # Test the network\n",
    "    print(\"Predictions after training:\")\n",
    "    for x in X:\n",
    "        print(f\"Input: {x}, Output: {nn.forward(x)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddbd269-309b-48ac-ba8e-105b8a758438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
